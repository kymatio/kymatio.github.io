{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classification of spoken digit recordings\n\nIn this example we use the 1D scattering transform to represent spoken digits,\nwhich we then classify using a simple classifier. This shows that 1D scattering\nrepresentations are useful for this type of problem.\n\nThis dataset is automatically downloaded and preprocessed from\nhttps://github.com/Jakobovski/free-spoken-digit-dataset.git\n\nDownloading and precomputing scattering coefficients should take about 5 min.\nRunning the gradient descent takes about 1 min.\n\nResults:\nTraining accuracy = 99.7%\nTesting accuracy = 98.0%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries\n\nSince we're using PyTorch to train the model, import `torch`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be constructing a logistic regression classifier on top of the\nscattering coefficients, so we need some of the neural network tools from\n`torch.nn` and the Adam optimizer from `torch.optim`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear, NLLLoss, LogSoftmax, Sequential\nfrom torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle audio file I/O, we import `os` and `scipy.io.wavfile`. We also need\n`numpy` for some basic array manipulation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.io import wavfile\nimport os\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate our results, we need to form a confusion matrix using\nscikit-learn and display them using `matplotlib`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we import the `Scattering1D` class from the `kymatio.torch` package\nand the `fetch_fsdd` function from `kymatio.datasets`. The `Scattering1D`\nclass is what lets us calculate the scattering transform, while the\n`fetch_fsdd` function downloads the FSDD, if needed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from kymatio.torch import Scattering1D\nfrom kymatio.datasets import fetch_fsdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline setup\nWe start by specifying the dimensions of our processing pipeline along with\nsome other parameters.\n\nFirst, we have signal length. Longer signals are truncated and shorter\nsignals are zero-padded. The sampling rate is 8000 Hz, so this corresponds to\nlittle over a second.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "T = 2**13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maximum scale 2**J of the scattering transform (here, about 30 milliseconds)\nand the number of wavelets per octave.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "J = 8\nQ = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need a small constant to add to the scattering coefficients before\ncomputing the logarithm. This prevents very large values when the scattering\ncoefficients are very close to zero.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_eps = 1e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If a GPU is available, let's use it!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reproducibility, we fix the seed of the random number generator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\nOnce the parameter are set, we can start loading the data into a format that\ncan be fed into the scattering transform and then a logistic regression\nclassifier.\n\nWe first download the dataset. If it's already downloaded, `fetch_fsdd` will\nsimply return the information corresponding to the dataset that's already\non disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "info_data = fetch_fsdd()\nfiles = info_data['files']\npath_dataset = info_data['path_dataset']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up Tensors to hold the audio signals (`x_all`), the labels (`y_all`), and\nwhether the signal is in the train or test set (`subset`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_all = torch.zeros(len(files), T, dtype=torch.float32, device=device)\ny_all = torch.zeros(len(files), dtype=torch.int64, device=device)\nsubset = torch.zeros(len(files), dtype=torch.int64, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each file in the dataset, we extract its label `y` and its index from the\nfilename. If the index is between 0 and 4, it is placed in the test set, while\nfiles with larger indices are used for training. The actual signals are\nnormalized to have maximum amplitude one, and are truncated or zero-padded\nto the desired length `T`. They are then stored in the `x_all` Tensor while\ntheir labels are in `y_all`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for k, f in enumerate(files):\n    basename = f.split('.')[0]\n\n    # Get label (0-9) of recording.\n    y = int(basename.split('_')[0])\n\n    # Index larger than 5 gets assigned to training set.\n    if int(basename.split('_')[2]) >= 5:\n        subset[k] = 0\n    else:\n        subset[k] = 1\n\n    # Load the audio signal and normalize it.\n    _, x = wavfile.read(os.path.join(path_dataset, f))\n    x = np.asarray(x, dtype='float')\n    x /= np.max(np.abs(x))\n\n    # Convert from NumPy array to PyTorch Tensor.\n    x = torch.from_numpy(x).to(device)\n\n    # If it's too long, truncate it.\n    if x.numel() > T:\n        x = x[:T]\n\n    # If it's too short, zero-pad it.\n    start = (T - x.numel()) // 2\n\n    x_all[k,start:start + x.numel()] = x\n    y_all[k] = y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log-scattering transform\nWe now create the `Scattering1D` object that will be used to calculate the\nscattering coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scattering = Scattering1D(J, T, Q).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the scattering transform for all signals in the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_all = scattering.forward(x_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since it does not carry useful information, we remove the zeroth-order\nscattering coefficients, which are always placed in the first channel of\nthe scattering Tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_all = Sx_all[:,1:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To increase discriminability, we take the logarithm of the scattering\ncoefficients (after adding a small constant to make sure nothing blows up\nwhen scattering coefficients are close to zero). This is known as the\nlog-scattering transform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_all = torch.log(torch.abs(Sx_all) + log_eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we average along the last dimension (time) to get a time-shift\ninvariant representation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_all = torch.mean(Sx_all, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the classifier\nWith the log-scattering coefficients in hand, we are ready to train our\nlogistic regression classifier.\n\nFirst, we extract the training data (those for which `subset` equals `0`)\nand the associated labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_tr, y_tr = Sx_all[subset == 0], y_all[subset == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Standardize the data to have mean zero and unit variance. Note that we need\nto apply the same transformation to the test data later, so we save the\nmean and standard deviation Tensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mu_tr = Sx_tr.mean(dim=0)\nstd_tr = Sx_tr.std(dim=0)\nSx_tr = (Sx_tr - mu_tr) / std_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define a logistic regression model using PyTorch. We train it using\nAdam with a negative log-likelihood loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_input = Sx_tr.shape[-1]\nnum_classes = y_tr.cpu().unique().numel()\nmodel = Sequential(Linear(num_input, num_classes), LogSoftmax(dim=1))\noptimizer = Adam(model.parameters())\ncriterion = NLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we're on a GPU, transfer the model and the loss function onto the device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\ncriterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training the model, we set some parameters for the optimization\nprocedure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of signals to use in each gradient descent step (batch).\nbatch_size = 32\n# Number of epochs.\nnum_epochs = 50\n# Learning rate for Adam.\nlr = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given these parameters, we compute the total number of batches.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nsamples = Sx_tr.shape[0]\nnbatches = nsamples // batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're ready to train the classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for e in range(num_epochs):\n    # Randomly permute the data. If necessary, transfer the permutation to the\n    # GPU.\n    perm = torch.randperm(nsamples, device=device)\n\n    # For each batch, calculate the gradient with respect to the loss and take\n    # one step.\n    for i in range(nbatches):\n        idx = perm[i * batch_size : (i+1) * batch_size]\n        model.zero_grad()\n        resp = model.forward(Sx_tr[idx])\n        loss = criterion(resp, y_tr[idx])\n        loss.backward()\n        optimizer.step()\n\n    # Calculate the response of the training data at the end of this epoch and\n    # the average loss.\n    resp = model.forward(Sx_tr)\n    avg_loss = criterion(resp, y_tr)\n\n    # Try predicting the classes of the signals in the training set and compute\n    # the accuracy.\n    y_hat = resp.argmax(dim=1)\n    accuracy = (y_tr == y_hat).float().mean()\n\n    print('Epoch {}, average loss = {:1.3f}, accuracy = {:1.3f}'.format(\n        e, avg_loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our network is trained, let's test it!\n\nFirst, we extract the test data (those for which `subset` equals `1`) and the\nassociated labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_te, y_te = Sx_all[subset == 1], y_all[subset == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the mean and standard deviation calculated on the training data to\nstandardize the testing data, as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Sx_te = (Sx_te - mu_tr) / std_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the response of the classifier on the test data and the resulting\nloss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resp = model.forward(Sx_te)\navg_loss = criterion(resp, y_te)\n\n# Try predicting the labels of the signals in the test data and compute the\n# accuracy.\n\ny_hat = resp.argmax(dim=1)\naccu = (y_te == y_hat).float().mean()\n\nprint('TEST, average loss = {:1.3f}, accuracy = {:1.3f}'.format(\n      avg_loss, accu))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the classification accuracy as a confusion matrix\nLet's see what the very few misclassified sounds get misclassified as. We\nwill plot a confusion matrix which indicates in a 2D histogram how often\none sample was mistaken for another (anything on the diagonal is correctly\nclassified, anything off the diagonal is wrong).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_categories = y_hat.cpu().numpy()\nactual_categories = y_te.cpu().numpy()\n\nconfusion = confusion_matrix(actual_categories, predicted_categories)\nplt.figure()\nplt.imshow(confusion)\ntick_locs = np.arange(10)\nticks = ['{}'.format(i) for i in range(1, 11)]\nplt.xticks(tick_locs, ticks)\nplt.yticks(tick_locs, ticks)\nplt.ylabel(\"True number\")\nplt.xlabel(\"Predicted number\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classification on CIFAR10 (ResNet)\n\nBased on pytorch example for CIFAR10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim\nfrom torchvision import datasets, transforms\nfrom kymatio.torch import Scattering2D\nimport kymatio.datasets as scattering_datasets\nimport argparse\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Scattering2dResNet(nn.Module):\n    def __init__(self, in_channels,  k=2, n=4, num_classes=10):\n        super(Scattering2dResNet, self).__init__()\n        self.inplanes = 16 * k\n        self.ichannels = 16 * k\n        self.K = in_channels\n        self.init_conv = nn.Sequential(\n            nn.BatchNorm2d(in_channels, eps=1e-5, affine=False),\n            nn.Conv2d(in_channels, self.ichannels,\n                  kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(self.ichannels),\n            nn.ReLU(True)\n        )\n\n        self.layer2 = self._make_layer(BasicBlock, 32 * k, n)\n        self.layer3 = self._make_layer(BasicBlock, 64 * k, n)\n        self.avgpool = nn.AdaptiveAvgPool2d(2)\n        self.fc = nn.Linear(64 * k * 4, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.view(x.size(0), self.K, 8, 8)\n        x = self.init_conv(x)\n\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\n\ndef train(model, device, train_loader, optimizer, epoch, scattering):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(scattering(data))\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 50 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader, scattering):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(scattering(data))\n            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\nif __name__ == '__main__':\n\n    \"\"\"Train a simple Hybrid Resnet Scattering + CNN model on CIFAR.\n\n        scattering 1st order can also be set by the mode\n        Scattering features are normalized by batch normalization.\n        The model achieves around 88% testing accuracy after 10 epochs.\n\n        scatter 1st order +\n        scatter 2nd order + linear achieves 70.5% in 90 epochs\n\n        scatter + cnn achieves 88% in 15 epochs\n\n    \"\"\"\n    parser = argparse.ArgumentParser(description='CIFAR scattering  + hybrid examples')\n    parser.add_argument('--mode', type=int, default=1,help='scattering 1st or 2nd order')\n    parser.add_argument('--width', type=int, default=2,help='width factor for resnet')\n    args = parser.parse_args()\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if args.mode == 1:\n        scattering = Scattering2D(J=2, shape=(32, 32), max_order=1)\n        K = 17*3\n    else:\n        scattering = Scattering2D(J=2, shape=(32, 32))\n        K = 81*3\n    scattering = scattering.to(device)\n\n\n\n\n    model = Scattering2dResNet(K, args.width).to(device)\n\n    # DataLoaders\n    num_workers = 4\n    if use_cuda:\n        pin_memory = True\n    else:\n        pin_memory = False\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, transform=transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(32, 4),\n            transforms.ToTensor(),\n            normalize,\n        ]), download=True),\n        batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n\n    # Optimizer\n    lr = 0.1\n    for epoch in range(0, 90):\n        if epoch%20==0:\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n                                        weight_decay=0.0005)\n            lr*=0.2\n\n        train(model, device, train_loader, optimizer, epoch+1, scattering)\n        test(model, device, test_loader, scattering)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classification on CIFAR10 (ResNet)\n\nBased on pytorch example for CIFAR10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nfrom kymatio import Scattering2D\nimport torch\nimport argparse\nimport kymatio.datasets as scattering_datasets\nimport torch.nn as nn\nfrom numpy.random import RandomState\nimport numpy as np\n\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n    def forward(self, x):\n        return x\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Scattering2dResNet(nn.Module):\n    def __init__(self, in_channels,  k=2, n=4, num_classes=10,standard=False):\n        super(Scattering2dResNet, self).__init__()\n        self.inplanes = 16 * k\n        self.ichannels = 16 * k\n        if standard:\n\n            self.init_conv = nn.Sequential(\n                nn.Conv2d(3, self.ichannels,\n                          kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(self.ichannels),\n                nn.ReLU(True)\n            )\n            self.layer1 = self._make_layer(BasicBlock, 16 * k, n)\n            self.standard = True\n        else:\n            self.K = in_channels\n            self.init_conv = nn.Sequential(\n                nn.BatchNorm2d(in_channels, eps=1e-5, affine=False),\n                nn.Conv2d(in_channels, self.ichannels,\n                      kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(self.ichannels),\n                nn.ReLU(True)\n            )\n            self.standard = False\n\n        self.layer2 = self._make_layer(BasicBlock, 32 * k, n)\n        self.layer3 = self._make_layer(BasicBlock, 64 * k, n)\n        self.avgpool = nn.AdaptiveAvgPool2d(2)\n        self.fc = nn.Linear(64 * k * 4, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        if not self.standard:\n            x = x.view(x.size(0), self.K, 8, 8)\n\n        x = self.init_conv(x)\n\n        if self.standard:\n            x = self.layer1(x)\n\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\n\ndef train(model, device, train_loader, optimizer, epoch, scattering):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(scattering(data))\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 50 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader, scattering):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(scattering(data))\n            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    \"\"\"Train a simple Hybrid Resnet Scattering + CNN model on CIFAR.\n\n    \"\"\"\n    parser = argparse.ArgumentParser(description='CIFAR scattering  + hybrid examples')\n    parser.add_argument('--mode', type=str, default='scattering',choices=['scattering', 'standard'],\n                        help='network_type')\n    parser.add_argument('--num_samples', type=int, default=50,\n                        help='samples per class')\n    parser.add_argument('--learning_schedule_multi', type=int, default=10,\n                        help='samples per class')\n    parser.add_argument('--seed', type=int, default=0,\n                        help='seed for dataset subselection')\n    parser.add_argument('--width', type=int, default=2,help='width factor for resnet')\n    args = parser.parse_args()\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if args.mode == 'scattering':\n        scattering = Scattering2D(J=2, shape=(32, 32))\n        K = 81*3\n        model = Scattering2dResNet(K, args.width).to(device)\n        scattering = scattering.to(device)\n    else:\n        model = Scattering2dResNet(8, args.width,standard=True).to(device)\n        scattering = Identity()\n\n\n    # DataLoaders\n    num_workers = 4\n    if use_cuda:\n        pin_memory = True\n    else:\n        pin_memory = False\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n\n    #####cifar data\n    cifar_data = datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, transform=transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(32, 4),\n            transforms.ToTensor(),\n            normalize,\n        ]), download=True)\n    # Extract a subset of X samples per class\n    prng = RandomState(args.seed)\n    random_permute = prng.permutation(np.arange(0, 5000))[0:args.num_samples]\n    indx = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute] for classe in range(0, 10)])\n\n    cifar_data.data, cifar_data.targets = cifar_data.data[indx], list(np.array(cifar_data.targets)[indx])\n    train_loader = torch.utils.data.DataLoader(cifar_data,\n                                               batch_size=32, shuffle=True, num_workers=num_workers,\n                                               pin_memory=pin_memory)\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n\n\n\n    # Optimizer\n    lr = 0.1\n    M = args.learning_schedule_multi\n    drops = [60*M,120*M,160*M]\n    for epoch in range(0, 200*M):\n        if epoch in drops or epoch==0:\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n                                        weight_decay=0.0005)\n            lr*=0.2\n\n        train(model, device, train_loader, optimizer, epoch+1, scattering)\n        if epoch%10==0:\n            test(model, device, test_loader, scattering)\n\n\n\nif __name__ == '__main__':\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}